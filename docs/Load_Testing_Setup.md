# Load Testing Setup

This document describes the steps and permissions required to run a load test that sends a specified load to IoT Hub that is already deployed in a specific subscription.

## Pre-requisites

The following pre-requisites are required to run the load test pipeline in a specific Azure DevOps subscription against a "system under test" (in this case, this represents the IoT Hub that is deployed in a specific subscription)

- AZURERM_SERVICE_CONNECTION_NAME
  
  A Service Connection to a subscription that has Contributor rights to create resource groups in that subscription

- SUT_SERVICE_CONNECTION_NAME

  A Service Connection to a subscription that has Read rights to generate a SAS token for the IoT Hub

- AZDO_ORG/AZDO_PROJECT/REPOSITORY_NAME/REPOSITORY_BRANCH

  These are the details of the Azure DevOps organization and repository details of the pipeline YAML file.

  For eg: if a developer wants to run the load test from their own Azure DevOps Organization, they can copy load_tests folder into their repository and configure a pipeline.

## Steps to provision a load test pipeline

Note: This step is required only if the pipeline is not yet configured in the Azure DevOps organization. Skip this if your Azure DevOps organization already has the pipeline

Install the [azure-devops Azure CLI extension](https://docs.microsoft.com/en-us/azure/devops/cli/?view=azure-devops)

- Open `load_tests/scripts/config.sh` and fill the required details.

- Navigate to `load_tests/scripts` and run `bash setup_project.sh`

This will setup the pipeline and its variables in the specified Azure DevOps organization.

## Pipeline Variables

- PROFILE_CONFIG

This points to a locust profile under the `load_tests/config/profiles` folder. This is useful to define a specific pattern of load test.

For eg: a burst profile has the following variables

```bash

# A test profile specifically for testing the pipeline E2E, just deploys one locust worker and runs test for two minutes
locust_step_load     = false
locust_users         = 1
locust_hatch_rate    = 1
locust_run_time      = "2m"
locust_workers_count = 1

```

If the pipeline needs to be run with a "storm" profile where it needs to generate a lot more load with lot more workers, the above variable values can be changed and stored as `storm.tfvars`

Profile Config provides a way to create different test profiles and through pipeline variables, a developer can change the profile easily.

- TARGET_CONFIG

This points to a target test config in `load_tests/config/targets/` folder. This is a file to add any environment variables etc that the locust test might itself require.

For eg: if you want to provide an environment variable to change the IOT Hub name, you can provide that as a variable here and use it in the test. It enables a developer to dynamically change any test related values.

- LOCUST_TEST_NAME

Name of the locust test in the `load_tests/tests` folder.

- AZURERM_SERVICE_CONNECTION_NAME

Name of the service connection that has contributor rights in a specific subscription, so that it can create resource groups.

- SUT_SERVICE_CONNECTION_NAME

Name of the "system under test" service connection that has Read rights in a subscription that has the system under test. In this case, it is the subscription that has the IoT Hub provisioned in it.

- IOT_HUB_NAME

Name of the IOT Hub in the subscription containing the system under test.

The rest of the variables are self explanatory.

## Running the pipeline

After the pipeline variables described above are filled in with the required values, run the Azure DevOps pipeline.

The pipeline provisions a number of Azure Container Instances as provided by the PROFILE_CONFIG and runs the tests across these container instances.

## Artifacts

The pipeline runs the tests in Azure Container Instances and downloads the following artifacts

`logs/`

Logs of Azure Container Instances

`results/`

CSV Log files generated by Locust

In the `stats.csv`, we can see the requests per second field to confirm if it generated the load we require.

## Cleanup Load Test Infrastructure

The pipeline has a destroy task to cleanup all the load testing resources that were deployed like Azure Container Instances, storage etc.

## Analyzing and Visualizing Load Test Results

The load test results can be analyzed through the following Application Insights queries:

### Function Instance Count Scaleout

```sql
requests
| where timestamp between (todatetime('2020-10-07 5:30 am')..todatetime('2020-10-07 6:00 am')) // start/end time of test
| summarize count() by cloud_RoleInstance, bin(timestamp, 1min)
| summarize ['instances'] = count() by bin(timestamp, 1min)
| render timechart
```

### Get the operationId for the event that has max elapsedTime

```sql
customEvents
| where timestamp between (todatetime('2020-10-07 5:30 am')..todatetime('2020-10-07 6:00 am')) // start/end time of test
| where name == "IoTAlertCreated"
| extend elapsedTime = todecimal(customMeasurements.AlertCreatedElapsedTimeMs)
| summarize arg_max(elapsedTime,operation_Id)
```

### Trace a specific operationId from service bus -> Functions -> Dynamics Dependencies

```sql
union *
| order by timestamp asc
| where operation_Id == "398b64c641b43243988248ec5ea7a40b"
```

### Avg/Min/Max/Percentiles for ElapsedTime for Work Order creation

Note: Visualize this as a unstacked column chart

```sql
customEvents
| where timestamp between (todatetime('2020-10-07 5:30 am')..todatetime('2020-10-07 6:00 am')) // start/end time of test
| where name == "IoTAlertCreated"
| extend elapsedTime = todecimal(customMeasurements.AlertCreatedElapsedTimeMs)
| summarize avg(elapsedTime),max(elapsedTime),min(elapsedTime), percentiles(elapsedTime,25,50,75,99) by name
```

### Time elapsed between the event entering IoT Hub and reaching functions should have an SLA of < 5 mins

```sql
customEvents
| where timestamp between (todatetime('2020-10-07 5:30 am')..todatetime('2020-10-07 6:00 am')) // start/end time of test
| where name == "IoTAlertCreated"
| extend elapsedTime = todecimal(customMeasurements.AlertCreatedElapsedTimeMs)
| where elapsedTime > 300000
```

### Exception Reporting

```sql
exceptions
| where timestamp between (todatetime('2020-10-07 5:30 am')..todatetime('2020-10-07 6:00 am')) // start/end time of test
```

### Average time taken by all Dynamics Dependencies

```sql
dependencies
| where timestamp between (todatetime('2020-10-07 5:30 am')..todatetime('2020-10-07 6:00 am')) // start/end time of test
| summarize avg(duration) by name
```

### Average Function Execution Time (FuncExecutionTimeinMS metric)

```sql
requests
| where timestamp between (todatetime('2020-10-07 5:30 am')..todatetime('2020-10-07 6:00 am')) // start/end time of test
| summarize appinsights_funcexecutiontime = avg(toreal(customDimensions.['FunctionExecutionTimeMs'])) by bin(timestamp,1m)
| render timechart
```

### Service Bus Messages In/Messages Out

The Azure Portal Metrics tab for Service Bus shows Incoming and Outgoing Messages.

For more details on logging and querying logs refer to [this document](./Logging.md)
